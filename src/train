#!/usr/bin/env python3
from __future__ import print_function
import os
import sys
import traceback

from tensorflow.python.client import device_lib

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
import numpy as np
import time
import json

import pandas as pd 
import re
from bs4 import BeautifulSoup
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
import warnings
pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore")

from attention import AttetionLayer

contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",
                           "didn't": "did not",  "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",
                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",
                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",
                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",
                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",
                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",
                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",
                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",
                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",
                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",
                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",
                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",
                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",
                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",
                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",
                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",
                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",
                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",
                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",
                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",
                           "you're": "you are", "you've": "you have"}

def read_hyperparameters():
    global batch_size
    global epochs
    print("Reading hyperparameters")
    with open(param_path, 'r') as tc:
        hyperparameters = json.load(tc)
    if "batch_size" in hyperparameters:
        batch_size = int(hyperparameters["batch_size"])
    if "epochs" in hyperparameters:
        epochs = int(hyperparameters["epochs"])

def read_prepare_data_for_training():
    import boto3
    import pandas as pd
    from sagemaker import get_execution_role
    role = get_execution_role()
    bucket='keras-sagemaker-train-abstractive'
    data_key = 'Reviews.csv'
    data_location = 's3://{}/data/{}'.format(bucket, data_key)
    data=pd.read_csv("Reviews.csv",nrows=10000)
    
    data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates
    data.dropna(axis=0,inplace=True)#dropping na
    
    stop_words = set(stopwords.words('english'))
    def text_cleaner(text,num):
        newString = text.lower()
        newString = BeautifulSoup(newString, "lxml").text
        newString = re.sub(r'\([^)]*\)', '', newString)
        newString = re.sub('"','', newString)
        newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(" ")])    
        newString = re.sub(r"'s\b","",newString)
        newString = re.sub("[^a-zA-Z]", " ", newString) 
        newString = re.sub('[m]{2,}', 'mm', newString)
        if(num==0):
            tokens = [w for w in newString.split() if not w in stop_words]
        else:
            tokens=newString.split()
        long_words=[]
        for i in tokens:
            if len(i)>1:                                                 #removing short word
                long_words.append(i)   
        return (" ".join(long_words)).strip()
    
    #call the function
    cleaned_text = []
    for t in data['Text']:
        cleaned_text.append(text_cleaner(t,0))
        
    #call the function
    cleaned_summary = []
    for t in data['Summary']:
        cleaned_summary.append(text_cleaner(t,1))
    data['cleaned_text']=cleaned_text
    data['cleaned_summary']=cleaned_summary
    
    data.replace('', np.nan, inplace=True)
    data.dropna(axis=0,inplace=True)
    
    cnt=0
    for i in data['cleaned_summary']:
        if(len(i.split())<=8):
            cnt=cnt+1
    print(cnt/len(data['cleaned_summary']))
    
    max_text_len=30
    max_summary_len=8
    
    cleaned_text =np.array(data['cleaned_text'])
    cleaned_summary=np.array(data['cleaned_summary'])
    short_text=[]
    short_summary=[]
    
    for i in range(len(cleaned_text)):
        if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):
            short_text.append(cleaned_text[i])
            short_summary.append(cleaned_summary[i])
    df=pd.DataFrame({'text':short_text,'summary':short_summary})
    
    df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')
    
    from sklearn.model_selection import train_test_split
    x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)
    
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    #prepare a tokenizer for reviews on training data
    x_tokenizer = Tokenizer() 
    x_tokenizer.fit_on_texts(list(x_tr))
    
    thresh=4
    cnt=0
    tot_cnt=0
    freq=0
    tot_freq=0
    for key,value in x_tokenizer.word_counts.items():
        tot_cnt=tot_cnt+1
        tot_freq=tot_freq+value
        if(value<thresh):
            cnt=cnt+1
            freq=freq+value
    print("% of rare words in vocabulary:",(cnt/tot_cnt)*100)
    print("Total Coverage of rare words:",(freq/tot_freq)*100)
    
    #prepare a tokenizer for reviews on training data
    x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) 
    x_tokenizer.fit_on_texts(list(x_tr))
    #convert text sequences into integer sequences
    x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) 
    x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)
    #padding zero upto maximum length
    x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')
    x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')
    #size of vocabulary ( +1 for padding token)
    x_voc   =  x_tokenizer.num_words + 1
    #prepare a tokenizer for reviews on training data
    y_tokenizer = Tokenizer()   
    y_tokenizer.fit_on_texts(list(y_tr))
    
    thresh=6
    cnt=0
    tot_cnt=0
    freq=0
    tot_freq=0
    for key,value in y_tokenizer.word_counts.items():
        tot_cnt=tot_cnt+1
        tot_freq=tot_freq+value
        if(value<thresh):
            cnt=cnt+1
            freq=freq+value
    print("% of rare words in vocabulary:",(cnt/tot_cnt)*100)
    print("Total Coverage of rare words:",(freq/tot_freq)*100)
    
    #prepare a tokenizer for reviews on training data
    y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) 
    y_tokenizer.fit_on_texts(list(y_tr))
    #convert text sequences into integer sequences
    y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) 
    y_val_seq   =   y_tokenizer.texts_to_sequences(y_val)
    #padding zero upto maximum length
    y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')
    y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')
    #size of vocabulary
    y_voc  =   y_tokenizer.num_words +1
    
    y_tokenizer.word_counts['sostok'],len(y_tr)
    
    ind=[]
    for i in range(len(y_tr)):
        cnt=0
        for j in y_tr[i]:
            if j!=0:
                cnt=cnt+1
        if(cnt==2):
            ind.append(i)
    y_tr=np.delete(y_tr,ind, axis=0)
    x_tr=np.delete(x_tr,ind, axis=0)
    
    ind1=[]
    for i in range(len(y_val)):
        cnt=0
        for j in y_val[i]:
            if j!=0:
                cnt=cnt+1
        if(cnt==2):
            ind.append(i)
    y_val=np.delete(y_val,ind, axis=0)
    x_val=np.delete(x_val,ind, axis=0)
    
    return x_tr, x_val, y_tr, y_val, x_voc, y_voc
    

def train_model(x_tr, x_val, y_tr, y_val, x_voc, y_voc):
    print("Starting the model training")
    from keras import backend as K
    K.clear_session()
    latent_dim = 300
    embedding_dim=100
    
    # Encoder
    encoder_inputs = Input(shape=(max_text_len,))
    
    #embedding layer
    enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)
    
    #encoder lstm 1
    encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)
    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)
    
    #encoder lstm 2
    encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)
    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)
    
    #encoder lstm 3
    encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)
    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)

    # Set up the decoder, using `encoder_states` as initial state.
    decoder_inputs = Input(shape=(None,))
    
    #embedding layer
    dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)
    dec_emb = dec_emb_layer(decoder_inputs)
    
    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)
    decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])
    
    # Attention layer
    attn_layer = AttentionLayer(name='attention_layer')
    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])
    # Concat attention input and decoder LSTM output
    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])
    
    #dense layer
    decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))
    decoder_outputs = decoder_dense(decoder_concat_input)

    # Define the model 
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    
    model.summary() 
    
    model.compile(loss=keras.losses.sparse_categorical_crossentropy,
                  optimizer=keras.optimizers.RMSprop(),
                  metrics=['accuracy'])
    
    # model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',)
    
    # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)
    
    model.fit([x_tr,y_tr[:,:-1]],
              y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],
              epochs=50,
              callbacks=[es],
              batch_size=128,
              validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))

    model.summary()

    #model.fit(x=x_train,
    #          y=y_train,
    #         callbacks=[es]
    #          batch_size=batch_size,
    #          epochs=epochs,
    #          verbose=1,
    #          validation_data=(x_test, y_test))

    score = model.evaluate([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:], verbose=0)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])

    with open(os.path.join(model_path, 'model_architecture.json'), 'w') as f:
        f.write(model.to_json())

    model.save(os.path.join(model_path, 'model.h5'))
    print("Finished training the model.")


def train():
    try:
        read_hyperparameters()
        x_tr, x_val, y_tr, y_val, x_voc, y_voc= read_prepare_data_for_training()
        train_model(x_train=x_tr, y_train=y_tr, x_test=x_val, y_test=y_val)
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)
    print("Finished training the model.")


if __name__ == '__main__':
    start = time.time()
    print(device_lib.list_local_devices())
    print("Script Status - Starting")
    train()
    print("Script Status - Finished")
    print("Total time taken to train the model: ", time.time() - start)

    # A zero exit code causes the job to be marked a succeeded.
    sys.exit(0)